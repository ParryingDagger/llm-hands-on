{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C6：提示工程\n",
    "\n",
    "## 文本生成模型\n",
    "\n",
    "### 选择模型\n",
    "\n",
    "从小的开源模型入手，例如：`microsoft/Phi-3-mini-4k-instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13daae62c76944d8896937d29fa89f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from util import init_torch_device\n",
    "\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "device = init_torch_device()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device.type,\n",
    "    torch_dtype='auto',\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \" Prompt engineering is like being a chef who knows exactly how to ask a recipe to make the perfect dish. Just as a chef chooses the right ingredients and follows the steps carefully, prompt engineering involves crafting the right questions or instructions to get the best possible answers from a computer program. It's about knowing exactly what you want to achieve and how to communicate that to the computer in a way that it understands and responds effectively.\"}]\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain the concept of prompt engineering in a way that is easy to understand.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(messages)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Explain the concept of prompt engineering in a way that is easy to understand.<|end|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 控制模型输出\n",
    "\n",
    "`pipeline` 加载模型时，`do_sample`控制了是否进行采样。使用 `temperature` 和 `top_p` 控制输出时，必须配置 `do_sample=True`\n",
    "\n",
    "#### temperature\n",
    "\n",
    "决定生成文本的随机性或创造性。`temperature=0` 时每次都生成相同的响应，即总是选择可能性最大的词\n",
    "\n",
    "* 高 `temperature` (例如 0.8) 用于产生更多样化的输出\n",
    "* 低 `temperature` (例如 0.2) 用于产生更具确定性的输出\n",
    "\n",
    "#### top_p\n",
    "\n",
    "`top-p`采样，核采样（nucleus sampling）\n",
    "\n",
    "考虑概率最高的若干词元，直到达到累积的概率限制\n",
    "\n",
    "例如：`top_p=0.1`，模型会从概率最高的 token 开始考虑，直到累积概率达到 0.1\n",
    "\n",
    "#### top_k\n",
    "\n",
    "制导律可能性最大的 k 个 token\n",
    "\n",
    "#### 应用场景\n",
    "\n",
    "| 场景 | temperature | top_p | 说明 |\n",
    "|---|---|---|---|\n",
    "|头脑风暴|高|高|高随机性输出，且可能输出的 token 集合较大，生成结果高度多样化。往往富有创意且出人意料|\n",
    "|邮件生成|低|低|高稳定输出，词元集小。产生可预测、重点明确、保守的结果|\n",
    "|创意写作|高|低|高随机输出，词元集小。产生有创意、且保持连贯性|\n",
    "|翻译|低|高|高稳定输出，词元集大。更广泛的词元范围，翻译结果更具多样性|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prompt engineering is like being a chef who knows exactly what ingredients to mix to create the perfect meal. In this case, the chef is carefully crafting a question or a set of instructions (prompts) for a computer system like Chatbot or AI. The goal is to guide the AI to digest the information, process it, and then provide a helpful answer or carry out a task just like Chatbot did in our previous example. The art of prompt engineering lies in the ability to word these prompts effectively, making sure they align well with the context, tone, and expected response the AI is going to provide. Imagine it like setting the table before a fancy dinner party – you have to ensure everything is just right before your guests (in this case, the AI) arrive. Through the skills of constructing the right prompts, we can get the AI to serve exactly what we need, like Chatbot did with the trivia game.\n"
     ]
    }
   ],
   "source": [
    "output = pipe(\n",
    "    messages,\n",
    "    do_sample=True,\n",
    "    temperature=1,\n",
    ")\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prompt engineering is like giving clear directions to a robot. Imagine you're trying to get your robot friend to bake a cake. You'd tell it exactly what ingredients to use, how to mix them, and how long to bake it. Prompt engineering works the same way but with computers. When you ask a computer to do something, like write a story or translate a sentence, you give it a 'prompt.' That's like your directions for the computer. The more clear and detailed your prompt is, the better the computer can understand and do what you want. It's all about how you phrase your question or command, just like how you'd tell a robot how to bake the perfect cake.\n"
     ]
    }
   ],
   "source": [
    "output = pipe(\n",
    "    messages,\n",
    "    do_sample=True,\n",
    "    top_p=1,\n",
    ")\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提示工程\n",
    "\n",
    "### 基本要素\n",
    "\n",
    "### 基于指令的提示词\n",
    "\n",
    "常见任务：\n",
    "    * 监督分类\n",
    "    * 搜索\n",
    "    * 摘要\n",
    "    * 代码生成\n",
    "    * 命名实体识别\n",
    "\n",
    "一些技术：\n",
    "    * 具体性：准确描述要达到的目的\n",
    "    * 幻觉：要求知道答案才生成，否则回答不知道\n",
    "    * 顺序：在提示词的开头或结尾放置指令，对于长提示，中间往往被遗忘。LLM 通常关注开头（首位效应）和结尾（近因效应）\n",
    "\n",
    "### 高级提示工程\n",
    "\n",
    "高级组件：\n",
    "    * 角色定位：例如，你是一个物理学家\n",
    "    * 指令：要求具体，避免有太大解释空间\n",
    "    * 上下文：描述或背景等附加信息，“为什么提出这个指令”\n",
    "    * 格式：输出文本的格式\n",
    "    * 受众：生成文本的目标对象，输出的水平\n",
    "    * 语气：在生成文本中应该使用的语气\n",
    "    * 数据：与任务本身相关的主要数据\n",
    "\n",
    "迭代：调整提示中包含的组件以及组件的顺序\n",
    "\n",
    "注意：提示词及策略在不同模型上有不同的效果\n",
    "\n",
    "#### 上下文学习\n",
    "\n",
    "直接展示想要完成目标任务的具体示例\n",
    "\n",
    "样本数量：\n",
    "* 零样本提示\n",
    "* 单样本提示\n",
    "* 少样本提示\n",
    "\n",
    "通过 `role` 区分用户（`user`）和模型（`assistant`）\n",
    "\n",
    "#### 链式提示\n",
    "\n",
    "用一个提示词的输出作为下一个提示词的输入\n",
    "\n",
    "将问题分解为几个部分，逐个完成\n",
    "\n",
    "场景：\n",
    "    * 响应验证：对之前的输出做二次检查\n",
    "    * 并行提示：并行创建多个提示词，合并最终结果\n",
    "    * 写故事：将问题分解为多个组件来利用 LLM 写书或故事\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Name: ChatSage\n",
      "Slogan: \"Unleashing the power of AI for seamless communication.\"\n"
     ]
    }
   ],
   "source": [
    "# 链式提示\n",
    "product_prompt = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Create a name and slogan for a chatbot that leverages LLM'\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(product_prompt)\n",
    "product_description = output[0]['generated_text']\n",
    "print(product_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Introducing ChatSage, the ultimate AI-powered communication solution that revolutionizes the way you connect with others. With our cutting-edge technology, we unleash the power of AI to provide seamless and efficient communication, making it easier than ever to stay connected. Say goodbye to miscommunication and hello to a world of effortless conversations with ChatSage. Experience the future of communication today!\n"
     ]
    }
   ],
   "source": [
    "sales_prompt = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': f'Generate a very short sales pitch for the following product: {product_description}'\n",
    "    }\n",
    "]\n",
    "\n",
    "outputs = pipe(sales_prompt)\n",
    "sales_pitch = outputs[0]['generated_text']\n",
    "print(sales_pitch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用生成模型推理\n",
    "\n",
    "#### 思维链：先思考再回答\n",
    "\n",
    "展示**推理示例**，引导模型在回答中运用推理\n",
    "\n",
    "或者零样本的情况下，**引导推理**，例如：`Let's think setp-by-step`\n",
    "\n",
    "#### 自洽性：采样输出\n",
    "\n",
    "多次使用相同提示\n",
    "\n",
    "将占多数的结果作为最终答案\n",
    "\n",
    "#### 思维树：探索中间步骤\n",
    "\n",
    "Tree-fo-Thought, ToT\n",
    "\n",
    "面对需要多个推理步骤的问题时，将其分解为多个部分\n",
    "\n",
    "模型被提示探索当前问题的不同解决方案，然后，对最佳解决方案投票，并进行下一步\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The cafeteria started with 23 apples. They used 20 apples to make lunch, so they had 23 - 20 = 3 apples left. After buying 6 more apples, they now have 3 + 6 = 9 apples. The answer is 9.\n"
     ]
    }
   ],
   "source": [
    "cot_prompt = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?'\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': 'Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 balls. 5 + 6 = 11. The answer is 11.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?'\n",
    "    }\n",
    "]\n",
    "outputs = pipe(cot_prompt)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1: Start with the initial number of apples in the cafeteria, which is 23.\n",
      "\n",
      "Step 2: Subtract the number of apples used to make lunch, which is 20.\n",
      "23 - 20 = 3 apples remaining.\n",
      "\n",
      "Step 3: Add the number of apples bought, which is 6.\n",
      "3 + 6 = 9 apples.\n",
      "\n",
      "So, the cafeteria now has 9 apples.\n"
     ]
    }
   ],
   "source": [
    "zeroshot_cot_prompt = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Let\\'s think step by step.'\n",
    "    }\n",
    "]\n",
    "outputs = pipe(zeroshot_cot_prompt)\n",
    "print(outputs[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Expert 1:\n",
      "Step 1: Start with the initial number of apples, which is 23.\n",
      "\n",
      "Expert 2:\n",
      "Step 1: Subtract the number of apples used for lunch (20) from the initial number (23), resulting in 3 apples remaining.\n",
      "Step 2: Add the number of apples bought (6) to the remaining apples (3), resulting in a total of 9 apples.\n",
      "\n",
      "Expert 3:\n",
      "Step 1: Begin with the initial number of apples (23).\n",
      "Step 2: Subtract the number of apples used for lunch (20) from the initial number (23), resulting in 3 apples remaining.\n",
      "Step 3: Add the number of apples bought (6) to the remaining apples (3), resulting in a total of 9 apples.\n",
      "\n",
      "Discussion:\n",
      "All three experts arrived at the same answer, which is 9 apples. This indicates that their calculations and reasoning were correct. The cafeteria started with 23 apples, used 20 for lunch, and then bought 6 more, resulting in a total of 9 apples.\n"
     ]
    }
   ],
   "source": [
    "# 思维树\n",
    "zeroshot_tot_prompt = [{\n",
    "    'role': 'user',\n",
    "    'content': '''Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then sare it with the group.\n",
    "Then all experts will go on to the next step, etc. If any expert realizes they're wrong at any point then they leave. \n",
    "The question is 'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?'\n",
    "Make sure to discuss the results.\n",
    "    '''\n",
    "}]\n",
    "outputs = pipe(zeroshot_tot_prompt)\n",
    "print(outputs[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输出验证\n",
    "\n",
    "要求：\n",
    "  * 结构化输出\n",
    "  * 输出有效性\n",
    "  * 伦理\n",
    "  * 准确性\n",
    "\n",
    "方法：\n",
    "  * 示例\n",
    "  * 语法\n",
    "  * 微调\n",
    "\n",
    "### 示例\n",
    "\n",
    "提供少样本\n",
    "\n",
    "### 语法：约束采样\n",
    "\n",
    "利用生成模型验证自己的输出\n",
    "\n",
    "将输出作为新的提示词，基于预定义的规则进行验证\n",
    "\n",
    "### 微调\n",
    "\n",
    "微调模型输出层"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
