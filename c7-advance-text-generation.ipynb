{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C7：高级文本生成技术和工具\n",
    "\n",
    "* LangChain\n",
    "* DSPy\n",
    "* Haystack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckduckgo_search\n",
      "  Downloading duckduckgo_search-8.0.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: click>=8.1.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from duckduckgo_search) (8.2.1)\n",
      "Collecting primp>=0.15.0 (from duckduckgo_search)\n",
      "  Downloading primp-0.15.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting lxml>=5.3.0 (from duckduckgo_search)\n",
      "  Downloading lxml-5.4.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (3.5 kB)\n",
      "Downloading duckduckgo_search-8.0.2-py3-none-any.whl (18 kB)\n",
      "Downloading lxml-5.4.0-cp313-cp313-macosx_10_13_universal2.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m49.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:06\u001b[0m\n",
      "\u001b[?25hDownloading primp-0.15.0-cp38-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m52.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:02\u001b[0m00:07\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: primp, lxml, duckduckgo_search\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [duckduckgo_search]\n",
      "\u001b[1A\u001b[2KSuccessfully installed duckduckgo_search-8.0.2 lxml-5.4.0 primp-0.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install langchain langchain-community llama-cpp-python langgraph duckduckgo_search\n",
    "#%pip install \n",
    "#%pip install llama-cpp-python\n",
    "%pip install duckduckgo_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donwload model\n",
    "#!curl -LO 'https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 链\n",
    "\n",
    "### 单链架构\n",
    "\n",
    "提示词模板：`langchain.PromptTemplate`\n",
    "\n",
    "单链结构一般是一个提示词组件+模型\n",
    "\n",
    "### 多提示词链式架构\n",
    "\n",
    "前一个模型的输出，作为后一个的提示词的一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' there! I need help crafting a CMake script. It\\'s for configuring the installation details of my software project, and it should do several things:\\n\\n1. Set an install prefix if it hasn\\'t been defined by the user, defaulting to \"/usr/local\". Make sure to trim any trailing slashes from this path.\\n2. Define a configuration name based on the build type variable provided, stripping non-alphanumeric characters at the start, or leave it empty if none is given. Also, display this configuration name for clarity.\\n3. Allow setting an install component only if one has been specified by the user and inform about which component will be installed with a message.\\n4. Include support to optionally grant execute permissions on shared libraries upon installation, which should default to \"OFF.\"\\n5. Decide whether the installation is happening due to cross-compiling efforts, with \"FALSE\" as the standard case unless specified otherwise by an external condition.\\n\\nI\\'d appreciate it if you can draft this script for me, ensuring that all user inputs are respected and only default values are used when necessary.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path='Phi-3-mini-4k-instruct-fp16.gguf',\n",
    "    max_tokens=500,\n",
    "    n_ctx=2048,\n",
    "    seed=42,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "llm.invoke('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from deepseek import base_url, api_key\n",
    "\n",
    "\n",
    "deepseek_model = ChatOpenAI(\n",
    "    model='deepseek-chat',\n",
    "    base_url=base_url,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "#deepseek_model.invoke('你是一个物理学家。请用严谨的方式介绍量子力学。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/llama_cpp/llama.py:1240: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Why don't cats play poker in the jungle? Too many cheetahs! But seriously, why did the cat sit on the computer? Because it wanted to keep an eye on the mouse! (Just kidding!) Cats are just too good at being mysterious for that.\\n\\n(Note: The joke plays around with puns and light-hearted humor related to cats without offending anyone.)\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "template = '''<s><|user|>\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['input_prompt']\n",
    ")\n",
    "\n",
    "basic_chain = prompt | llm\n",
    "\n",
    "basic_chain.invoke({'input_prompt': 'Hello, please tell me a joke about cat.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/llama_cpp/llama.py:1240: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 1 + 1 equals 2. This is a basic arithmetic addition problem, where you combine one unit with another to get two units in total.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_chain.invoke({'input_prompt': 'What is 1 + 1?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class StoryTitle(TypedDict):\n",
    "    title: str\n",
    "    summary: str\n",
    "\n",
    "\n",
    "def process_title_output(output, config, *args, **kwargs):\n",
    "    print(f'output: {output}')\n",
    "    print(f'config: {config}')\n",
    "    print(f'args: {args}')\n",
    "    print(f'kwargs: {kwargs}')\n",
    "    return {\n",
    "        'title': output\n",
    "    }\n",
    "\n",
    "\n",
    "title_prompt = PromptTemplate(\n",
    "    template='''<s><|user|>\n",
    "    Create a title for a story about {summary}. Only return the title.<|end|>\n",
    "    <|assistant|>''',\n",
    "    input_variables=['summary']\n",
    ")\n",
    "title =  title_prompt | {'summary': RunnablePassthrough(), 'title': llm}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/llama_cpp/llama.py:1240: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': StringPromptValue(text='<s><|user|>\\n    Create a title for a story about A girl chosen to be the saintess for godness of beauty escaping from the destiny of being a sacrifice for the sin of lust. Only return the title.<|end|>\\n    <|assistant|>'),\n",
       " 'title': ' \"Belle\\'s Ascent: Defying Beauty\\'s Sacrifice\"'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title.invoke('A girl chosen to be the saintess for godness of beauty escaping from the destiny of being a sacrifice for the sin of lust')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 记忆\n",
    "\n",
    "<https://www.langchain.com.cn/docs/how_to/message_history/>\n",
    "\n",
    "### 对话缓冲区\n",
    "\n",
    "直接将完整对话记录注入输入提示\n",
    "\n",
    "控制窗口上下文：仅保留最近 k 轮的对话记录\n",
    "\n",
    "### 对话摘要\n",
    "\n",
    "通过另一个 LLM 模型接受完整的对话记录，提炼摘要，保留对话历史的核心\n",
    "\n",
    "使用历史摘要注入输入提示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/runnables/history.py:596: LangChainDeprecationWarning: `connection_string` was deprecated in LangChain 0.2.2 and will be removed in 1.0. Use connection instead.\n",
      "  message_history = self.get_session_history(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output_message': AIMessage(content='Hi Bob! 👋 Nice to meet you! How can I help you today? 😊', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 8, 'total_tokens': 27, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 8}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0425fp8', 'id': 'ca284ee6-c72f-4763-8316-d66d56c8e588', 'finish_reason': 'stop', 'logprobs': None}, id='run--32a757ad-6216-4833-8436-d7c03a8c5bff-0', usage_metadata={'input_tokens': 8, 'output_tokens': 19, 'total_tokens': 27, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RunnableWithMessageHistory\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    return SQLChatMessageHistory(session_id, \"sqlite:///memory.db\")\n",
    "\n",
    "\n",
    "chain = RunnableParallel({\"output_message\": deepseek_model})\n",
    "\n",
    "\n",
    "runnable_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    output_messages_key=\"output_message\",\n",
    ")\n",
    "\n",
    "runnable_with_history.invoke(\n",
    "    [HumanMessage(content=\"hi - im bob!\")],\n",
    "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent：智能体\n",
    "\n",
    "agent: 可以自主规划行动及其序列的系统\n",
    "\n",
    "ReAct（reasoning and acting，推理与行动）框架\n",
    "\n",
    "### 工具\n",
    "\n",
    "<https://www.langchain.com.cn/docs/how_to/custom_tools/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "Multiply two numbers.\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "# Let's inspect some of the attributes associated with the tool.\n",
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "async def amultiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "calculator = StructuredTool.from_function(func=multiply, coroutine=amultiply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9x/vrm2qywn327gf_tw665dc9w40000gn/T/ipykernel_5729/2858425346.py:2: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  title_tool = title.as_tool()\n"
     ]
    }
   ],
   "source": [
    "# runnable as_tool\n",
    "title_tool = title.as_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impl BaseTool\n",
    "from typing import Optional, Type\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "\n",
    "class CalculatorInput(BaseModel):\n",
    "    a: int = Field(description=\"first number\")\n",
    "    b: int = Field(description=\"second number\")\n",
    "\n",
    "\n",
    "# Note: It's important that every field has type hints. BaseTool is a\n",
    "# Pydantic class and not having type hints can lead to unexpected behavior.\n",
    "class CustomCalculatorTool(BaseTool):\n",
    "    name: str = \"Calculator\"\n",
    "    description: str = \"useful for when you need to answer questions about math\"\n",
    "    args_schema: Type[BaseModel] = CalculatorInput\n",
    "    return_direct: bool = True\n",
    "\n",
    "    def _run(\n",
    "        self, a: int, b: int, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        return a * b\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        a: int,\n",
    "        b: int,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        # If the calculation is cheap, you can just delegate to the sync implementation\n",
    "        # as shown below.\n",
    "        # If the sync calculation is expensive, you should delete the entire _arun method.\n",
    "        # LangChain will automatically provide a better implementation that will\n",
    "        # kick off the task in a thread to make sure it doesn't block other async code.\n",
    "        return self._run(a, b, run_manager=run_manager.get_sync())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_agent_action callback: AttributeError(\"'str' object has no attribute 'log'\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'tool'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m agent = create_react_agent(deepseek_model, tools, prompt=prompt)\n\u001b[32m     22\u001b[39m agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the current price of a MacBook Pro in CNY? How much would it cost in EUR with current exchange rate?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py:1620\u001b[39m, in \u001b[36mAgentExecutor._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m   1618\u001b[39m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_continue(iterations, time_elapsed):\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m     next_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1627\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[32m   1628\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._return(\n\u001b[32m   1629\u001b[39m             next_step_output, intermediate_steps, run_manager=run_manager\n\u001b[32m   1630\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py:1328\u001b[39m, in \u001b[36mAgentExecutor._take_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_next_step\u001b[39m(\n\u001b[32m   1318\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1319\u001b[39m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1323\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1324\u001b[39m ) -> Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[32m   1325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consume_next_step(\n\u001b[32m   1326\u001b[39m         \u001b[43m[\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1336\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py:1411\u001b[39m, in \u001b[36mAgentExecutor._iter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1409\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[32m-> \u001b[39m\u001b[32m1411\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\n\u001b[32m   1413\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/agents/agent.py:1425\u001b[39m, in \u001b[36mAgentExecutor._perform_agent_action\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[39m\n\u001b[32m   1423\u001b[39m     run_manager.on_agent_action(agent_action, color=\u001b[33m\"\u001b[39m\u001b[33mgreen\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1424\u001b[39m \u001b[38;5;66;03m# Otherwise we lookup the tool\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43magent_action\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool\u001b[49m \u001b[38;5;129;01min\u001b[39;00m name_to_tool_map:\n\u001b[32m   1426\u001b[39m     tool = name_to_tool_map[agent_action.tool]\n\u001b[32m   1427\u001b[39m     return_direct = tool.return_direct\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'tool'"
     ]
    }
   ],
   "source": [
    "# ReAct\n",
    "from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchTool\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "from langgraph.prebuilt.chat_agent_executor import create_react_agent\n",
    "from langchain.agents.agent import AgentExecutor\n",
    "# LLMMath已经废弃\n",
    "tools = load_tools([], llm=deepseek_model)\n",
    "tools.append(DuckDuckGoSearchTool())\n",
    "tools.append(calculator)\n",
    "\n",
    "react_template = \"\"\"\n",
    "\n",
    "Question: {messages}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=react_template,\n",
    "    input_variables=[\"messages\"]\n",
    ")\n",
    "\n",
    "\n",
    "agent = create_react_agent(deepseek_model, tools, prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What is the current price of a MacBook Pro in CNY? How much would it cost in EUR with current exchange rate?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
