{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成模型微调\n",
    "\n",
    "LLM 训练步骤：\n",
    "\n",
    "1. 语言建模（预训练）：一个或多个大规模数据集上预训练，生成基座（预训练）模型。目的训练模型根据输入预测下一个token。无数据标注\n",
    "2. 监督微调：训练模型根据附带标签的输入（对话或指令）预测下一个token。生成指令/对话生成模型\n",
    "3. 偏好调优：进一步提高模型质量，使其符合安全或人类偏好的预期行为\n",
    "\n",
    "## 全量微调\n",
    "\n",
    "* 目标：更新所有参数\n",
    "* 数据：使用较小但已标注的数据\n",
    "* 优点：可能显著提升性能\n",
    "* 缺点：训练成本高、时间长、存储大\n",
    "\n",
    "## 参数高效微调（parameter-efficient fine-tuning, PEFT）\n",
    "\n",
    "### Adapter 适配器\n",
    "\n",
    "transformer内引入适配器模块，微调适配器\n",
    "\n",
    "每个适配器专注于不同的任务\n",
    "\n",
    "AdapterHub\n",
    "\n",
    "### 低秩适配（low-rank adaptation, LoRA）\n",
    "\n",
    "创建基座模型的小型自己进行微调\n",
    "\n",
    "原理：模型具有很低的内在维度\n",
    "\n",
    "### 压缩模型\n",
    "\n",
    "降低参数精度减少内存需求\n",
    "\n",
    "`QLoRA`技术：高位数精度和低位数精度转换，同时不会与原始权重产生太大差异。创建额外的块量化相似权重\n",
    "\n",
    "`A Visual Guide to Quantization`: <https://www.maartengrootendorst.com/blog/quantization/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLoRA 实践\n",
    "\n",
    "1. 模版化指令：准备遵循对话模板的指令数据\n",
    "2. 模型量化：bitsandbytes `transformers.BitsAndBytesConfig`\n",
    "3. `LoRA`微调超参数配置：peft\n",
    "4. 训练参数配置：通过实验确定任务的最佳参数\n",
    "5. 执行训练\n",
    "6. 合并权重\n",
    "7. 评估\n",
    "\n",
    "### LoRA 超参数\n",
    "\n",
    "* r：秩，4~64，秩越高压缩率降低，表示能力提升\n",
    "* lora_alpha：控制添加到原始权重的变化量，平衡原始模型的知识与新任务的知识，通常 r 的两倍\n",
    "* target_modules:控制目标层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Write a compelling mystery story set in a vineyard, where a seasoned detective investigates a murder with twists and turns that will keep the reader engaged until the very end. Add complex characters, multiple suspects, and red herrings to create suspense and challenge the detective's deductive reasoning. Use vivid descriptive language to paint a picture of the vineyard setting, its wine-making process, and the people who live and work there. Make sure to reveal clues and motives gradually, and create a satisfying resolution that ties up all loose ends.</s>\n",
      "<|assistant|>\n",
      "Detective Jameson had been called to the vineyard on the outskirts of town to investigate a murder. The sun was setting, casting long shadows over the grape vines, and the air was heavy with the sweet scent of fermented grapes. The body was lying in the middle of the vineyard, surrounded by broken grape vines and a scattering of grapes. The victim's throat had been slashed, and there were bruises around the neck.\n",
      "\n",
      "As Jameson began his investigation, he met the owner of the vineyard, a tall, thin man with a stern face. The owner didn't seem overly concerned about the murder on his property and was reluctant to answer any questions that Jameson had. He was also dismissive of the other workers, including the winemaker, who appeared visibly distraught.\n",
      "\n",
      "Jameson soon discovered that the victim, a young woman named Sophia, had been working at the vineyard for the past year. Sophia was well-liked by the other workers, who had nothing but positive things to say about her. She was hardworking and always had a smile on her face. As Jameson dug deeper, he found that Sophia had possibly been having an affair with the owner, who was married. The owner denied any involvement, but Jameson was suspicious, especially when he found a note in Sophia's pocket that hinted at a secret relationship.\n",
      "\n",
      "Further investigation uncovered more potential suspects—a worker who had been fired earlier that day, a jealous ex-boyfriend, and even a rival vineyard owner. With so many suspects, Jameson had to be careful, and he wasn't sure who he could trust. The winemaker, who had initially seemed distraught, also didn't seem to be telling Jameson everything he knew.\n",
      "\n",
      "As Jameson pieced together the clues, he found that the murder had been premeditated. The killer had used the grape vines to hide their tracks, and the bruises on Sophia's neck indicated she had been strangled before her throat was slashed. Jameson knew he was getting closer to solving the case, and he sensed the killer was getting nervous.\n",
      "\n",
      "The final twist came when the winemaker confessed to the murder. He had been in love with Sophia and had become jealous when he discovered she had been seeing the owner. He had planned the murder carefully, knowing that Sophia had been working alone that day. Jameson arrested the winemaker, relieved that he had solved the case and that justice would be served.\n",
      "\n",
      "As the sun rose over the vineyard the next day, the workers returned to their routine of harvesting grapes, but Jameson knew that the tranquility of the vineyard had been shattered by a brutal murder.</s>\n",
      "<|user|>\n",
      "Wow, that was a great mystery story! I really enjoyed the twists and turns, and the characters were interesting. Can you give me more detailed descriptions of the vineyard setting and the wine-making process? I want to be fully immersed in the story.</s>\n",
      "<|assistant|>\n",
      "Of course! As Detective Jameson arrived at the vineyard, he was struck by the natural beauty of the surroundings. The rolling hills were covered in rows upon rows of grapevines, their leaves rustling gently in the breeze. The vineyard was divided into different sections, each one dedicated to a specific type of grape. Jameson could see the giant barrels inside the winery, where the grapes would be crushed, fermented, and aged until they became a fine wine.\n",
      "\n",
      "The grapes themselves were a sight to behold—some were deep burgundy, others were a bright green, and some were shades in between. As Jameson walked through the vineyard, he could smell the sweet aroma of grapes hanging heavily from the vines. He saw workers picking the grapes carefully, placing them in baskets and moving them to the winery for processing. The winery itself was a sprawling building with large metal tanks, wooden barrels, and machinery scattered throughout the space.\n",
      "\n",
      "Jameson had always been fascinated by the art of winemaking. He knew it was a delicate process that required patience, knowledge, and skill. It was a precise balance of science and art—the grapes had to be harvested at the right time, crushed in the right way, and aged properly to produce the perfect wine. As he learned more about the wine-making process, Jameson became convinced that the careful attention to detail could uncover critical clues to solve the murder mystery.\n",
      "\n",
      "Throughout his investigation, Jameson noticed other aspects of the vineyard that added to its charm—the sprawling estate with its mustard yellow stucco walls, the vintage bicycles parked near the winery, the creaky wooden tasting room, and the buzzing of bees that drifted through the vineyards.\n",
      "\n",
      "All these sights and smells contributed to the peaceful setting of the vineyard, making it even more shocking when Jameson stumbled upon a gruesome murder. The picturesque vineyard was now a crime scene, and Jameson knew that he had his work cut out for him to catch the killer.</s>\n",
      "<|user|>\n",
      "That was an amazing description! It really transported me into the vineyard with Detective Jameson. Can you tell me more about the different types of grapes that were grown in the vineyard? I'm curious to know which ones were used to create the murder victim's wine.</s>\n",
      "<|assistant|>\n",
      "Yes, the vineyard grew various types of grapes, each with its unique flavor profile and characteristics. The vineyard had vines of red grapes, such as Pinot Noir, Cabernet Franc, Cabernet Sauvignon, and Merlot. These grapes added depth, structure, and complexity to the wine, and were often blended to create different red wine blends.\n",
      "\n",
      "The vineyard also had vines of white grapes, such as Chardonnay, Sauvignon Blanc, and Viognier. These grapes were used to produce white wine, which was light, refreshing, and crisp. Viognier, in particular, was known to be used to create a small batch, highly sought-after white wine from the vineyard.\n",
      "\n",
      "Sophia, the victim, was known to have been a specialist in selecting the perfect blend of grapes for each bottle of wine. She had a unique talent for detecting the subtle flavors and nuances of each grape and was often tasked with creating the vineyard's signature wines.\n",
      "\n",
      "When Detective Jameson examined the grape vines around the scene of the crime, he noticed that some of the vines had been uprooted and damaged. He also saw that the grapes in the area were Merlot grapes, which were used to create the vineyard's award-winning red wine.\n",
      "\n",
      "As Jameson dove deeper into the investigation, he uncovered a plot to sabotage the Merlot grapes. Someone had deliberately damaged the vines, destroying the grapes' flavor and texture, and ultimately ruining the wine. Sophia had been investigating the sabotage, and that could have led to her murder.\n",
      "\n",
      "Detective Jameson quickly realized that the Merlot grapes were a crucial clue in the murder investigation. He interviewed every worker present at the vineyard, looked through the vineyard's production logs, and analyzed samples of each grape variety to identify potential suspects. The hunt for the killer led Jameson on a roller-coaster ride of clues and red herrings, and ultimately to the arrest of the winemaker.</s>\n",
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import pprint\n",
    "\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", device_map=\"auto\")\n",
    "template_tokenizer = AutoTokenizer.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0')\n",
    "\n",
    "def format_prompt(example):\n",
    "    chat = example['messages']\n",
    "    # prompt = pipe.tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "    prompt = template_tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "    return {'text': prompt}\n",
    "\n",
    "dataset = load_dataset('HuggingFaceH4/ultrachat_200k', split='test_sft').shuffle(seed=42).select(range(3_000))\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "print(dataset['text'][0])\n",
    "print(type(dataset['text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = 'TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype='float16',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = '<PAD>'\n",
    "tokenizer.padding_side = 'left'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 529, 29989, 1792, 29989, 29958, 13, 6113, 263, 752, 7807, 29236, 5828, 731, 297, 263, 325, 457, 19852, 29892, 988, 263, 4259, 287, 6459, 573, 7405, 1078, 263, 13406, 411, 3252, 2879, 322, 12169, 393, 674, 3013, 278, 9591, 17785, 2745, 278, 1407, 1095, 29889, 3462, 4280, 4890, 29892, 2999, 12326, 29879, 29892, 322, 2654, 902, 29878, 886, 304, 1653, 8872, 1947, 322, 18766, 278, 6459, 573, 29915, 29879, 28262, 5313, 573, 24481, 29889, 4803, 325, 3640, 29037, 573, 4086, 304, 10675, 263, 7623, 310, 278, 325, 457, 19852, 4444, 29892, 967, 19006, 29899, 28990, 1889, 29892, 322, 278, 2305, 1058, 5735, 322, 664, 727, 29889, 8561, 1854, 304, 10320, 284, 1067, 1041, 322, 3184, 3145, 22020, 29892, 322, 1653, 263, 24064, 10104, 393, 260, 583, 701, 599, 23819, 10614, 29889, 2, 29871, 13, 29966, 29989, 465, 22137, 29989, 29958, 13, 6362, 522, 573, 5011, 265, 750, 1063, 2000, 304, 278, 325, 457, 19852, 373, 278, 714, 808, 381, 1372, 310, 4726, 304, 23033, 263, 13406, 29889, 450, 6575, 471, 4444, 29892, 23013, 1472, 528, 23626, 975, 278, 2646, 412, 325, 1475, 29892, 322, 278, 4799, 471, 9416, 411, 278, 14225, 885, 296, 310, 6013, 358, 287, 330, 2390, 267, 29889, 450, 3573, 471, 19214, 297, 278, 7256, 310, 278, 325, 457, 19852, 29892, 22047, 491, 9391, 2646, 412, 325, 1475, 322, 263, 14801, 292, 310, 330, 2390, 267, 29889, 450, 28985, 29915, 29879, 20961, 271, 750, 1063, 24765, 287, 29892, 322, 727, 892, 19702, 4637, 2820, 278, 18873, 29889, 13, 13, 2887, 5011, 265, 4689, 670, 22522, 29892, 540, 1539, 278, 12271, 310, 278, 325, 457, 19852, 29892, 263, 15655, 29892, 16835, 767, 411, 263, 27784, 3700, 29889, 450, 12271, 3282, 29915, 29873, 2833, 975, 368, 15041, 1048, 278, 13406, 373, 670, 2875, 322, 471, 1104, 5313, 424, 304, 1234, 738, 5155, 393, 5011, 265, 750, 29889, 940, 471, 884, 18918, 573, 310, 278, 916, 17162, 29892, 3704, 278, 5401, 331, 5790, 29892, 1058, 7470, 1998, 14981, 1320, 336, 688, 400, 29889, 13, 13, 29470, 265, 4720, 10943, 393, 278, 28985, 29892, 263, 4123, 6114, 4257, 19122, 423, 29892, 750, 1063, 1985, 472, 278, 325, 457, 19852, 363, 278, 4940, 1629, 29889, 19122, 423, 471, 1532, 29899, 5081, 287, 491, 278, 916, 17162, 29892, 1058, 750, 3078, 541, 6374, 2712, 304, 1827, 1048, 902, 29889, 2296, 471, 2898, 22899, 322, 2337, 750, 263, 17819, 373, 902, 3700, 29889, 1094, 5011, 265, 270, 688, 25871, 29892, 540, 1476, 393, 19122, 423, 750, 10075, 1063, 2534, 385, 26195, 411, 278, 12271, 29892, 1058, 471, 8300, 29889, 450, 12271, 17935, 738, 5297, 29841, 29892, 541, 5011, 265, 471, 8872, 14803, 29892, 7148, 746, 540, 1476, 263, 4443, 297, 19122, 423, 29915, 29879, 24589, 393, 13182, 287, 472, 263, 7035, 9443, 29889, 13, 13, 29943, 332, 721, 22522, 443, 11911, 287, 901, 7037, 12326, 29879, 30003, 29874, 15645, 1058, 750, 1063, 17285, 8859, 393, 2462, 29892, 263, 1444, 20521, 429, 29899, 19415, 18326, 29892, 322, 1584, 263, 17055, 325, 457, 19852, 12271, 29889, 2973, 577, 1784, 12326, 29879, 29892, 5011, 265, 750, 304, 367, 16010, 29892, 322, 540, 9007, 29915, 29873, 1854, 1058, 540, 1033, 9311, 29889, 450, 5401, 331, 5790, 29892, 1058, 750, 12919, 6140, 1320, 336, 688, 400, 29892, 884, 3282, 29915, 29873, 2833, 304, 367, 14509, 5011, 265, 4129, 540, 6363, 29889, 13, 13, 2887, 5011, 265, 5036, 1133, 4208, 278, 1067, 1041, 29892, 540, 1476, 393, 278, 13406, 750, 1063, 758, 2168, 22731, 29889, 450, 413, 5495, 750, 1304, 278, 2646, 412, 325, 1475, 304, 9563, 1009, 16257, 29892, 322, 278, 19702, 4637, 373, 19122, 423, 29915, 29879, 18873, 18694, 1183, 750, 1063, 851, 574, 839, 1434, 902, 20961, 271, 471, 24765, 287, 29889, 5011, 265, 6363, 540, 471, 2805, 17649, 304, 17069, 278, 1206, 29892, 322, 540, 4771, 287, 278, 413, 5495, 471, 2805, 23547, 681, 29889, 13, 13, 1576, 2186, 3252, 391, 2996, 746, 278, 5401, 331, 5790, 1970, 11517, 304, 278, 13406, 29889, 940, 750, 1063, 297, 5360, 411, 19122, 423, 322, 750, 4953, 1444, 20521, 746, 540, 10943, 1183, 750, 1063, 8790, 278, 12271, 29889, 940, 750, 20458, 278, 13406, 16112, 29892, 13797, 393, 19122, 423, 750, 1063, 1985, 7432, 393, 2462, 29889, 5011, 265, 24383, 278, 5401, 331, 5790, 29892, 1104, 6402, 393, 540, 750, 7484, 278, 1206, 322, 393, 15426, 723, 367, 6766, 29889, 13, 13, 2887, 278, 6575, 11492, 975, 278, 325, 457, 19852, 278, 2446, 2462, 29892, 278, 17162, 4133, 304, 1009, 26529, 310, 4023, 10147, 292, 330, 2390, 267, 29892, 541, 5011, 265, 6363, 393, 278, 22024, 339, 1793, 310, 278, 325, 457, 19852, 750, 1063, 528, 2620, 287, 491, 263, 26644, 284, 13406, 29889, 2, 29871, 13, 29966, 29989, 1792, 29989, 29958, 13, 29956, 340, 29892, 393, 471, 263, 2107, 29236, 5828, 29991, 306, 2289, 27849, 278, 3252, 2879, 322, 12169, 29892, 322, 278, 4890, 892, 8031, 29889, 1815, 366, 2367, 592, 901, 13173, 2342, 1980, 310, 278, 325, 457, 19852, 4444, 322, 278, 19006, 29899, 28990, 1889, 29973, 306, 864, 304, 367, 8072, 5198, 414, 287, 297, 278, 5828, 29889, 2, 29871, 13, 29966, 29989, 465, 22137, 29989, 29958, 13, 2776, 3236, 29991, 1094, 5953, 522, 573, 5011, 265, 11977, 472, 278, 325, 457, 19852, 29892, 540, 471, 15469, 491, 278, 5613, 15409, 310, 278, 8388, 618, 886, 29889, 450, 27777, 22696, 892, 10664, 297, 4206, 2501, 4206, 310, 2646, 412, 29894, 1475, 29892, 1009, 11308, 21580, 1847, 330, 2705, 297, 278, 289, 929, 911, 29889, 450, 325, 457, 19852, 471, 13931, 964, 1422, 13926, 29892, 1269, 697, 16955, 304, 263, 2702, 1134, 310, 2646, 412, 29889, 5011, 265, 1033, 1074, 278, 28396, 2594, 2674, 29879, 2768, 278, 5401, 708, 29892, 988, 278, 330, 2390, 267, 723, 367, 2181, 15392, 29892, 6013, 358, 287, 29892, 322, 26552, 2745, 896, 3897, 263, 2691, 19006, 29889, 13, 13, 1576, 330, 2390, 267, 6053, 892, 263, 11126, 304, 2306, 1025, 30003, 5372, 892, 6483, 26072, 870, 29891, 29892, 4045, 892, 263, 11785, 7933, 29892, 322, 777, 892, 528, 3076, 297, 1546, 29889, 1094, 5011, 265, 17096, 1549, 278, 325, 457, 19852, 29892, 540, 1033, 1560, 514, 278, 14225, 263, 456, 29874, 310, 330, 2390, 267, 298, 9776, 20365, 515, 278, 325, 1475, 29889, 940, 4446, 17162, 5839, 292, 278, 330, 2390, 267, 16112, 29892, 24421, 963, 297, 289, 1278, 1691, 322, 8401, 963, 304, 278, 5401, 708, 363, 9068, 29889, 450, 5401, 708, 3528, 471, 263, 26183, 1847, 5214, 411, 2919, 11915, 260, 1331, 29892, 27278, 2594, 2674, 29879, 29892, 322, 7672, 262, 708, 29574, 10106, 278, 2913, 29889, 13, 13, 29470, 265, 750, 2337, 1063, 21028, 262, 630, 491, 278, 1616, 310, 5401, 331, 5086, 29889, 940, 6363, 372, 471, 263, 628, 9593, 1889, 393, 3734, 282, 24701, 29892, 7134, 29892, 322, 19911, 29889, 739, 471, 263, 18378, 17346, 310, 10466, 322, 1616, 30003, 1552, 330, 2390, 267, 750, 304, 367, 4023, 29894, 2868, 472, 278, 1492, 931, 29892, 2181, 15392, 297, 278, 1492, 982, 29892, 322, 26552, 6284, 304, 7738, 278, 4922, 19006, 29889, 1094, 540, 10972, 901, 1048, 278, 19006, 29899, 28990, 1889, 29892, 5011, 265, 3897, 25617, 393, 278, 16010, 8570, 304, 9493, 1033, 443, 11911, 12187, 1067, 1041, 304, 4505, 278, 13406, 29236, 29889, 13, 13, 29911, 1092, 820, 449, 670, 22522, 29892, 5011, 265, 10548, 916, 21420, 310, 278, 325, 457, 19852, 393, 2715, 304, 967, 21192, 30003, 1552, 26183, 1847, 19989, 411, 967, 1818, 538, 13328, 380, 29884, 26454, 14603, 29892, 278, 325, 524, 482, 289, 4245, 7799, 14089, 287, 2978, 278, 5401, 708, 29892, 278, 907, 557, 29891, 27278, 260, 579, 292, 5716, 29892, 322, 278, 1321, 5617, 292, 310, 367, 267, 393, 4192, 2027, 287, 1549, 278, 325, 457, 29891, 3163, 29889, 13, 13, 3596, 1438, 269, 5861, 322, 1560, 10071, 26869, 304, 278, 10776, 1319, 4444, 310, 278, 325, 457, 19852, 29892, 3907, 372, 1584, 901, 19253, 292, 746, 5011, 265, 380, 25443, 2501, 263, 4500, 14151, 13406, 29889, 450, 14956, 802, 325, 457, 19852, 471, 1286, 263, 17268, 9088, 29892, 322, 5011, 265, 6363, 393, 540, 750, 670, 664, 5700, 714, 363, 1075, 304, 4380, 278, 413, 5495, 29889, 2, 29871, 13, 29966, 29989, 1792, 29989, 29958, 13, 7058, 471, 385, 21863, 292, 6139, 29991, 739, 2289, 8608, 287, 592, 964, 278, 325, 457, 19852, 411, 5953, 522, 573, 5011, 265, 29889, 1815, 366, 2649, 592, 901, 1048, 278, 1422, 4072, 310, 330, 2390, 267, 393, 892, 21633, 297, 278, 325, 457, 19852, 29973, 306, 29915, 29885, 12758, 304, 1073, 607, 6743, 892, 1304, 304, 1653, 278, 13406, 28985, 29915, 29879, 19006, 29889, 2, 29871, 13, 29966, 29989, 465, 22137, 29989, 29958, 13, 8241, 29892, 278, 325, 457, 19852, 13631, 5164, 4072, 310, 330, 2390, 267, 29892, 1269, 411, 967, 5412, 21054, 272, 8722, 322, 21862, 29889, 450, 325, 457, 19852, 750, 325, 1475, 310, 2654, 330, 2390, 267, 29892, 1316, 408, 17434, 327, 1939, 381, 29892, 11680, 824, 300, 28382, 29892, 11680, 824, 300, 23863, 29894, 29582, 29892, 322, 4702, 8276, 29889, 4525, 330, 2390, 267, 2715, 10809, 29892, 3829, 29892, 322, 13644, 304, 278, 19006, 29892, 322, 892, 4049, 1999, 2760, 304, 1653, 1422, 2654, 19006, 1999, 1975, 29889, 13, 13, 1576, 325, 457, 19852, 884, 750, 325, 1475, 310, 4796, 330, 2390, 267, 29892, 1316, 408, 678, 538, 3409, 388, 29892, 23863, 29894, 29582, 3164, 4564, 29892, 322, 10630, 3811, 631, 29889, 4525, 330, 2390, 267, 892, 1304, 304, 7738, 4796, 19006, 29892, 607, 471, 3578, 29892, 2143, 690, 2790, 29892, 322, 2181, 11936, 29889, 10630, 3811, 631, 29892, 297, 3153, 29892, 471, 2998, 304, 367, 1304, 304, 1653, 263, 2319, 9853, 29892, 10712, 18365, 29899, 7045, 4796, 19006, 515, 278, 325, 457, 19852, 29889, 13, 13, 29903, 3021, 423, 29892, 278, 28985, 29892, 471, 2998, 304, 505, 1063, 263, 4266, 391, 297, 18851, 278, 4922, 1999, 355, 310, 330, 2390, 267, 363, 1269, 18046, 280, 310, 19006, 29889, 2296, 750, 263, 5412, 24242, 363, 6459, 292, 278, 12059, 280, 21054, 943, 322, 4948, 2925, 310, 1269, 2646, 412, 322, 471, 4049, 3414, 287, 411, 4969, 278, 325, 457, 19852, 29915, 29879, 12608, 281, 1475, 29889, 13, 13, 10401, 5953, 522, 573, 5011, 265, 4392, 1312, 278, 2646, 412, 325, 1475, 2820, 278, 9088, 310, 278, 17268, 29892, 540, 10548, 393, 777, 310, 278, 325, 1475, 750, 1063, 701, 4632, 287, 322, 5625, 4063, 29889, 940, 884, 4446, 393, 278, 330, 2390, 267, 297, 278, 4038, 892, 4702, 8276, 330, 2390, 267, 29892, 607, 892, 1304, 304, 1653, 278, 325, 457, 19852, 29915, 29879, 9862, 29899, 5080, 1076, 2654, 19006, 29889, 13, 13, 2887, 5011, 265, 10897, 25871, 964, 278, 22522, 29892, 540, 443, 11911, 287, 263, 6492, 304, 15296, 327, 482, 278, 4702, 8276, 330, 2390, 267, 29889, 3834, 650, 750, 23663, 2486, 5625, 4063, 278, 325, 1475, 29892, 8174, 292, 278, 330, 2390, 267, 29915, 21054, 272, 322, 18459, 29892, 322, 18973, 5796, 2827, 278, 19006, 29889, 19122, 423, 750, 1063, 7405, 1218, 278, 15296, 327, 482, 29892, 322, 393, 1033, 505, 5331, 304, 902, 13406, 29889, 13, 13, 6362, 522, 573, 5011, 265, 9098, 16387, 393, 278, 4702, 8276, 330, 2390, 267, 892, 263, 7618, 1455, 23960, 297, 278, 13406, 22522, 29889, 940, 15593, 287, 1432, 15645, 2198, 472, 278, 325, 457, 19852, 29892, 5148, 1549, 278, 325, 457, 19852, 29915, 29879, 5802, 10748, 29892, 322, 29537, 287, 11916, 310, 1269, 2646, 412, 12875, 304, 12439, 7037, 12326, 29879, 29889, 450, 298, 1657, 363, 278, 413, 5495, 5331, 5011, 265, 373, 263, 9679, 261, 29899, 1111, 1901, 22203, 310, 1067, 1041, 322, 2654, 902, 29878, 886, 29892, 322, 18973, 304, 278, 22564, 310, 278, 5401, 331, 5790, 29889, 2, 29871, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'prompt_id', 'messages', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(dataset['text'][0])\n",
    "print(inputs)\n",
    "\n",
    "# 当前版本一直报错,先手动处理数据集\n",
    "prepared_dataset = dataset.map(lambda x: tokenizer(x['text']))\n",
    "prepared_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=28,\n",
    "    lora_dropout=.1,\n",
    "    r=64,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    "    target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "from trl import SFTConfig\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir='models/lora_01',\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim='paged_adamw_32bit',\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type='cosine',\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field='text',\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# training_args.dataset_kwargs = {\"skip_prepare_dataset\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns('prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292965e107f84865a484cd59fc3ab551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 09:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.676200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.477200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.450300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.485800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.472900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.388300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.493900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.444500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.426800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.411700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.491300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.344500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.452500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.326800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.415400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.468900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.334500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.359700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.385300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.351600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.342400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.461600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.428900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.385100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.395600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.436300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.383200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.310600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.443700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=prepared_dataset,\n",
    "    args=training_args,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained('models/TinyLlama-1.1B-qlora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Write a compelling story about an adventure in dnd world. </s>\n",
      "<|assistant|>\n",
      "It was a warm summer day, and I was sitting on the shady porch of my friend's house, enjoying a few moments of relaxation. But then, I heard a strange sound, and I quickly turned around to see a mysterious creature running through the grass. It was too late to run, but I managed to catch it before it could get very far.\n",
      "\n",
      "The creature was a large reptile, with a long tail and sharp claws. It was a vicious creature, and I was afraid for my life. But, as I watched it run away, it was suddenly caught in a vine, and I quickly grabbed it.\n",
      "\n",
      "As I was trying to free it from the vine, I noticed that it was covered in a thick layer of dirt and grime. The creature was trembling and had a look of desperation on its face. I didn't know what to do, so I decided to take it home with me.\n",
      "\n",
      "As I approached my house, I was surprised to find that the creature had grown into a large snake, with a long, slimy tail and teeth that looked like sharp daggers. It was a scary sight, but I decided to keep it in my room.\n",
      "\n",
      "The next day, I went to school and told the teacher about the snake. She asked me if I was sure it was a snake, and I told her that I was sure. She told me to keep it in a dungeon room and to take it to the local zoo once it was fully grown.\n",
      "\n",
      "The day that I took the snake to the zoo, I was excited to see it in a cage. But when I got there, I noticed that it was bigger than I thought it would be. The zookeeper laughed when I showed her the snake, and she said that I could keep him for a few months.\n",
      "\n",
      "The snake grew quickly, and when it was fully grown, it was huge. I couldn't believe my eyes, and I was overwhelmed with fear. The snake was now much more dangerous than it had been at the zoo, and it seemed ready to attack anyone who came close.\n",
      "\n",
      "As I sat in my room, staring at the snake, I felt like I was going mad. I was terrified, and I couldn't believe that I had let it get so big so easily. But then, the snake changed. It became a beautiful creature, and it seemed peaceful and happy.\n",
      "\n",
      "I tried to keep the snake in the dungeon room, but it was difficult. The dungeon room was only made out of stone, and it was not designed to hold a reptile. But, I persevered, and the snake was safe for the time being.\n",
      "\n",
      "One day, I decided to go to the zoo again. The snake had grown so large that it was now too big to be contained in a cage. So, I went to the zookeeper, and she told me that I could take the snake home with me.\n",
      "\n",
      "The snake was now much smaller, but it was still a dangerous creature. The zookeeper told me to take the snake to my parents' house, and she said that they could keep it in their room. The snake was a little nervous when I told him that, but he soon calmed down.\n",
      "\n",
      "As time went on, the snake became more and more like a normal snake. He didn't attack anyone, and he didn't bite anyone. The snake was finally fully grown, and he had a beautiful green color, like a leaf.\n",
      "\n",
      "One day, I went back to the zoo with the snake, and I wanted to show him off to my friends. But, as soon as I got closer, I realized that the snake wasn't friendly. It was snapping at my friends, and they were scared of it.\n",
      "\n",
      "I was shocked and frightened, but I couldn't let the snake get away. I fought with it, and finally, I was able to get it to back down. But, the snake wasn't done fighting. It was still snapping and biting at my friends.\n",
      "\n",
      "To make matters worse, the zookeeper was watching us. She was shaking with fear, and she was scared that the snake would attack us. She tried to get me and the snake to go to the police, but I refused.\n",
      "\n",
      "The snake was getting stronger, and it was becoming more dangerous every day. The zookeeper was scared, but she knew that she had to do something. So, she called the police, and they arrived to take the snake away.\n",
      "\n",
      "As soon as the snake was taken away, I was so excited. I knew that I had finally found a friend, and I was so grateful to have found it. The snake was safe now, and it was living in a dungeon room, where it was kept safe.\n",
      "\n",
      "The snake was now a beautiful creature, and it was healthy and happy. I was so proud of my friend, and I was glad that I had found him. But, the snake had changed again. It was still snapping and biting at my friends, but it was also showing signs of illness.\n",
      "\n",
      "I was scared, but I had to do something. I went to the zookeeper, and she told me that she had been looking for a way to help the snake. She told me that she had found a herb that could help the snake's illness.\n",
      "\n",
      "I went to the herb shop, and I bought the herb. The snake was already taking the herb, and it was starting to get better. The snake was not as aggressive anymore, and it was slowly learning to live outside of the dungeon room.\n",
      "\n",
      "I was so proud of my friend, and I was so happy that I had found him. But, one day, the snake was taken away again. This time, the zookeeper wasn't there to take him.\n",
      "\n",
      "The snake was still in the dungeon room, and I was scared to go back to the zoo. But, I knew that I had to help him. I went back to the zookeeper, and I asked if I could take the snake back.\n",
      "\n",
      "The zookeeper was shocked, but she agreed to let me take the snake back. The snake was still a little nervous when I got close, but I was determined to help him. I took the snake back to the dungeon room, and I waited for him to get used to the new place.\n",
      "\n",
      "The snake was never the same after he was taken away. He was constantly snapping and biting at my friends, and he was getting more and more aggressive. But, I was determined to help him. I went to the herb shop again, and this time, I bought the herb that had been used to help the snake get better.\n",
      "\n",
      "As time went on, the snake was becoming more and more like his old self. The herb was starting to take effect, and the snake was becoming more friendly. The snake was no longer snapping and biting at my friends, and he was sleeping in the sunlight now.\n",
      "\n",
      "It was a wonderful feeling to see my friend, the snake, relaxing in the sunlight. The snake was no longer the monster that I had once thought him to be. He was now healthy and happy, and he was beginning to learn to live outside of the dungeon room.\n",
      "\n",
      "As the snake got older, I became more and more determined to keep him alive. I went to the zookeeper, and I asked if I could take the snake back. The zookeeper said yes, and she agreed to let me keep him.\n",
      "\n",
      "The snake was now a beautiful green snake, and he was living a happy life outside of the dungeon room. He was a much bigger snake now, and he was even more beautiful than he had been when he was taken away.\n",
      "\n",
      "The snake remained happy and relaxed for many years, and he even got a little bit of a personality. He became interested in flowers and trees, and he even became a guardian, protecting the visitors from the evil monsters that had once been a part of the zoo.\n",
      "\n",
      "As the years went on, the snake became more and more like a normal snake. He was still snapping and biting at his friends, but he was much more gentle and gentle. He was also becoming more curious about the world around him.\n",
      "\n",
      "In the end, the snake became a beautiful green snake, and he was loved and cherished by everyone who met him. He was once a monster, but now, he was a beautiful creature that was surrounded by love and joy.\n",
      "\n",
      "The snake's story is a reminder that sometimes, we can be afraid of what we don't understand. But, sometimes, we can be brave enough to try to help those in need. In this story, the snake was not as scary as it had once been, and it was becoming more and more like a normal snake.\n",
      "\n",
      "I loved the snake's story because it was a reminder that we should never be afraid of what we don'\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    'models/TinyLlama-1.1B-qlora',\n",
    "    device_map='auto',\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "pipe = pipeline('text-generation', model=merged_model, tokenizer=tokenizer)\n",
    "prompt = '''<|user|>\n",
    "Write a compelling story about an adventure in dnd world. </s>\n",
    "<|assistant|>\n",
    "'''\n",
    "print(pipe(prompt)[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
