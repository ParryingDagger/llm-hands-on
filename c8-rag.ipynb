{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C8：语义搜索和 RAG\n",
    "\n",
    "语义搜索主流技术：\n",
    "  * 稠密搜索：基于文本嵌入，将搜索问题转化为查询向量与文档向量的最临近匹配过程\n",
    "  * 重排序：多阶段处理，重排序模型对结果做相关性评分，并优化排序\n",
    "  * RAG\n",
    "\n",
    "## 重排序\n",
    "\n",
    "1. 目标文本预处理和句子分割\n",
    "2. 生成句子的向量表示\n",
    "3. 简历搜索索引\n",
    "4. 执行搜索并分析结果\n",
    "\n",
    "缺点：\n",
    "  * 不存在答案时仍然返回结果：通常返回结果由用户自行判断，并根据反馈持续优化模型\n",
    "  * 无法精准匹配短语：需要结合关键字搜索\n",
    "  * 在训练数据外性能显著下降\n",
    "  * 长文本分块\n",
    "\n",
    "### 长文本分块策略\n",
    "\n",
    "\n",
    "#### 单文本单向量方案\n",
    "\n",
    "* 仅嵌入文档的代表性段落，忽略剩余内容：适合可以概括核心观点的文章\n",
    "* 分割后对各块做嵌入，聚合块嵌入到单个向量：压缩缺陷导致大量细节丢失\n",
    "\n",
    "#### 单文档多向量方案\n",
    "\n",
    "几种选择：\n",
    "\n",
    "* 句子分割：可能粒度过小，无法捕捉足够的上下文\n",
    "* 段落分割：段落较短时，或 3～8 个句子一个段落\n",
    "* 上下文增强：\n",
    "  * 块附加标题\n",
    "  * 重叠结构：引入一部分上下文内容\n",
    "* LLM 动态智能分块\n",
    "\n",
    "### 搜索\n",
    "\n",
    "* 搜索库：\n",
    "  * Annoy\n",
    "  * FAISS\n",
    "* 向量数据库：增删向量无需重建索引等高级功能\n",
    "\n",
    "### 微调\n",
    "\n",
    "核心：拉进相关查询和文档的距离，推离不相关查询\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from faiss-cpu) (2.2.4)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.11.0-cp313-cp313-macosx_14_0_arm64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m484.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install cohere rank_bm25\n",
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "\n",
    "api_key = ''\n",
    "\n",
    "co = cohere.Client(api_key)\n",
    "\n",
    "text = ''\n",
    "\n",
    "texts = [sentence.strip(' \\n') for sentence in text.split('.')]\n",
    "response = co.embed(texts, input_type='search_document').embeddings\n",
    "embeds = np.array(response)\n",
    "print(embeds.shape)\n",
    "\n",
    "dim = embeds.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "print(index.is_trained)\n",
    "index.add(np.float32(embeds))\n",
    "\n",
    "def search(query, k=3):\n",
    "    query_embed = co.embed(query, input_type='search_query').embeddings[0]\n",
    "    distances, indices = index.search(np.float32([query_embed]), k)\n",
    "    texts_np = np.array(texts)\n",
    "    results = pd.DataFrame(data={'texts': texts_np[indices[0]], 'distances': distances[0]})\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "query = ''\n",
    "results = search(query)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重排序\n",
    "\n",
    "先检索，一般采用关键字+稠密的混合方式\n",
    "\n",
    "再通过重排序模型计算相关性，并根据评分排序搜索结果\n",
    "\n",
    "### sentence-transformers\n",
    "\n",
    "<https://www.sbert.net/examples/sentence_transformer/applications/retrieve_rerank/README.html>\n",
    "\n",
    "### 工作机制\n",
    "\n",
    "将查询与每个候选结果共同输入交叉编码器架构的 LLM：允许模型同事分析查询文本与文档内容后生成相关性评分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ''\n",
    "\n",
    "results = co.rerank(query=query, documents=texts, top_n=3, return_documents=True)\n",
    "\n",
    "for idx, result in enumerate(results):\n",
    "    print(idx, result.relevance_score, result.document.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检索评估指标\n",
    "\n",
    "搜索系统评估框架三要素：\n",
    "  * 文档库\n",
    "  * 查询集合\n",
    "  * 相关性判断\n",
    "\n",
    "指标：\n",
    "  * 基于平均精确率的单查询评分\n",
    "  * 基于均值平均精确率的多查询评分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "检索 + 生成\n",
    "\n",
    "### 基于知识的生成\n",
    "\n",
    "核心：在搜索末端接入 LLM\n",
    "\n",
    "具体实现：将用户的问题与检索获得的前若干个相关文档共同输入 LLM，使其基于检索提供的上下文生成答案\n",
    "\n",
    "### 高级技术\n",
    "\n",
    "#### 查询改写\n",
    "\n",
    "使用 LLM 将原始查询改写成更利于 LLM 的简洁模式\n",
    "\n",
    "#### 多查询 RAG\n",
    "\n",
    "将复杂问题生成多个关联查询\n",
    "\n",
    "对多次查询最佳结果输入模型进行事实性回答\n",
    "\n",
    "或者给改写器自主判断能力：需要执行检索或直接生成可靠的答案\n",
    "\n",
    "#### 多跳 RAG\n",
    "\n",
    "分布复杂推离，执行连续检索\n",
    "\n",
    "#### 查询路由\n",
    "\n",
    "根据数据源做重定向检索\n",
    "\n",
    "#### Agent RAG\n",
    "\n",
    "数据源抽象为 Agent 的 tools\n",
    "\n",
    "### 效果评估\n",
    "\n",
    "核心维度：\n",
    "\n",
    "* 流畅性：生成文本的语言流畅度和实用价值\n",
    "* 感知效用：回答内容的信息价值和使用价值\n",
    "* 引用召回率：外部事实陈述中获得完整引证支持的比例\n",
    "* 引用精确率：引用内容对相关论断的支持的有效性\n",
    "\n",
    "#### Ragas\n",
    "\n",
    "开源工具库\n",
    "\n",
    "评估指标：\n",
    "\n",
    "* 重视度：答案和上下文一致性程度\n",
    "* 答案相关性：答案和提问主题的契合度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan.\n",
    "It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine.\n",
    "Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind.\n",
    "\n",
    "Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007.\n",
    "Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar.\n",
    "Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm.\n",
    "Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles.\n",
    "Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects.\n",
    "\n",
    "Interstellar premiered on October 26, 2014, in Los Angeles.\n",
    "In the United States, it was first released on film stock, expanding to venues using digital projectors.\n",
    "The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014.\n",
    "It received acclaim for its performances, direction, screenplay, musical score, visual effects, ambition, themes, and emotional weight.\n",
    "It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics. Since its premiere, Interstellar gained a cult following,[5] and now is regarded by many sci-fi experts as one of the best science-fiction films of all time.\n",
    "Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades\"\"\"\n",
    "\n",
    "\n",
    "texts = [sentence.strip(' \\n') for sentence in text.split('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'Income generated', 'result': \" I'm sorry, but there is no information provided above to answer the question about income generated. If you can provide details or context related to the income in question, I would be happy to help with an analysis or summary.\"}\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from langchain import LlamaCpp\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "#<https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html#langchain_community.vectorstores.faiss.FAISS>\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path='Phi-3-mini-4k-instruct-fp16.gguf',\n",
    "    max_tokens=500,\n",
    "    n_ctx=2048,\n",
    "    seed=42,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='BAAI/bge-small-en-v1.5'\n",
    ")\n",
    "\n",
    "index = faiss.IndexFlatL2(len(embedding_model.embed_query(\"hello world\")))\n",
    "\n",
    "db = FAISS(\n",
    "    embedding_function=embedding_model,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "template = '''<|user|>\n",
    "Relevant information:\n",
    "{context}\n",
    "\n",
    "Provide a concise answer the following question using the relevant information provided above:\n",
    "{question}<|end|>\n",
    "<|assistant|>\n",
    "'''\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['context', 'question']\n",
    ")\n",
    "\n",
    "rag = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=db.as_retriever(),\n",
    "    chain_type_kwargs={'prompt': prompt},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "query = 'Income generated'\n",
    "result = rag.invoke(query)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
